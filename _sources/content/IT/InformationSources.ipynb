{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{math}\n",
    "\\newcommand{\\snII}[5]{#1: \\left( \\begin{matrix} {#2} & {#4} \\\\ #3 & #5 \\end{matrix} \\right)}\n",
    "\\newcommand{\\snIII}[7]{#1: \\left( \\begin{matrix} {#2} & {#4} & {#6} \\\\ #3 & #5 & #7 \\end{matrix} \\right)}\n",
    "\\newcommand{\\snIV}[9]{#1:  \\left( \\begin{matrix} {#2} & {#4} & {#6} & {#8} \\\\ #3 & #5 & #7 & #9 \\end{matrix} \\right)}\n",
    "\n",
    "\\newcommand{\\sII}[3] {#1: \\left( \\begin{matrix} s_1 & s_2 \\\\ #2 & #3 \\end{matrix} \\right)}\n",
    "\\newcommand{\\sIII}[4] {#1: \\left( \\begin{matrix} s_1 & s_2 & s_3 \\\\ #2 & #3 & #4 \\end{matrix} \\right)}\n",
    "\\newcommand{\\sIV}[5] {#1: \\left( \\begin{matrix} s_1 & s_2 & s_3 & s_4 \\\\ #2 & #3 & #4  & #5 \\end{matrix} \\right)}\n",
    "\\newcommand{\\sVI}[7] {#1: \\left( \\begin{matrix} s_1 & s_2 & s_3 & s_4 & s_5 & s_6 \\\\ #2 & #3 & #4 & #5 & #6 & #7\\end{matrix} \\right)}\n",
    "\\newcommand{\\sVIII}[9] {#1: \\left( \\begin{matrix} s_1 & s_2 & s_3 & s_4 & s_5 & s_6 & s_7 & s_8\\\\ #2 & #3 & #4 & #5 & #6 & #7 & #8 & #9 \\end{matrix} \\right)}\n",
    "\\newcommand{\\fIoII}{\\frac{1}{2}}\n",
    "\\newcommand{\\fIoIII}{\\frac{1}{3}}\n",
    "\\newcommand{\\fIoIV}{\\frac{1}{4}}\n",
    "\\newcommand{\\fIoV}{\\frac{1}{5}}\n",
    "\\newcommand{\\fIoVI}{\\frac{1}{6}}\n",
    "\\newcommand{\\fIoVII}{\\frac{1}{7}}\n",
    "\\newcommand{\\fIoVIII}{\\frac{1}{8}}\n",
    "```\n",
    "\n",
    "# Discrete Information Sources\n",
    "\n",
    "## A mathematical definition of information\n",
    "\n",
    "In order to analyze information generation, encoding and transmission \n",
    "with mathematical tools,\n",
    "we need a solid and clear definition of information.\n",
    "\n",
    "<!-- ### Block diagram of a communication system\n",
    "\n",
    "![Block diagram of a communication system](img/CommBlockDiagram.png){width=50%}\n",
    "\n",
    "- Source: creates information messages \n",
    "- Encoder: converts messages into symbols for transmission (i.e bits)\n",
    "- Channel: delivers the symbols, introduces errors\n",
    "- Decoder: detects/corrects the errors, rebuilds the information messages -->\n",
    "\n",
    "### What is information?\n",
    "\n",
    "Let's start first with some simple examples of messages carrying information.\n",
    "\n",
    "Consider the sentence: \n",
    "\n",
    "> \"Poli Iași (the local football team) lost the last match\" \n",
    "\n",
    "Does this message carry information? How, why, how much?\n",
    "\n",
    "When thinking about the information brought by such a phrase, let's consider the following facts:\n",
    "\n",
    "  - the message carries information only when you don't already know the result\n",
    "  - if you already known the result, the message is useless (brings no information)\n",
    "  - if the result was to be expected, there is little information (e.g. suppose Poli Iași plays agains FC Barcelona)\n",
    "  - if the result is highly unusual, there is more information in this message\n",
    "\n",
    "```{sidebar} Example\n",
    "\n",
    "To understand why the amount of information \n",
    "depends on whether the result is to be expected or is highly unusual,\n",
    "consider a case when information is valuable, for example:\n",
    "\n",
    "- in an exam or quiz with multiple-choice answers (e.g. Who Wants to be a Millionaire)\n",
    "- in sports betting (gambling), \n",
    "\n",
    "The amount you gain depends on the amount of information you have on the topic: those who know a lot compared to others win a lot,\n",
    "those who know little win just a little. But you're not winning a fortune\n",
    "by answering easy questions which most people know, or betting on result which everybody expects.\n",
    "We explain this by the fact that there is little information involved in them.\n",
    "You do win a lot by answering hard questions or betting on surprising results,\n",
    "and since it's the information which is valuable, it shows that unexpected outcomes have lots of information.\n",
    "```\n",
    "\n",
    "#### Information and events\n",
    "\n",
    "As seen in the example above, the notion of information requires a probabilistic setting.\n",
    "\n",
    "We define the notion of **information** for a **probabilistic event**:\n",
    "\n",
    "- an event may or may not happen, and when it does happen, it creates information\n",
    "\n",
    "- the information is: \"this event happened\"\n",
    "\n",
    "- the amount of information depends on the **probability** of that event\n",
    "\n",
    "```{hint}\n",
    "As a rule of thumb, keep in mind:\n",
    "\n",
    "  - if you can guess something most of the times, it has little information\n",
    "  - if something is unexpected, it has a lot of information\n",
    "```\n",
    "\n",
    "```{admonition} Question\n",
    "Let's answer some questions:\n",
    "\n",
    "  - does a sure event (p = 1) bring any information?\n",
    "  - does an almost sure event (e.g. P = 0.9999)  bring little or much information?\n",
    "  - does a rare event (e.g. P = 0.0001) bring a little or much information?\n",
    "```\n",
    "\n",
    "#### Definition of information\n",
    "\n",
    "Throughout this class, we refer to a probabilistic event as a **\"message\"**\n",
    "\n",
    "The information attached to a message $s_i$ is rigorously defined as:\n",
    "$$i(s_i) = -\\log_2(p(s_i))$$\n",
    "\n",
    "Consequences of this definition:\n",
    "\n",
    "- Information of an event is always non-negative: \n",
    "\n",
    "$$\n",
    "i(s_i) \\geq 0\n",
    "$$\n",
    "\n",
    "- Lower probability (rare events) means higher information\n",
    "- Higher probability (frequent events) means lower information\n",
    "- The certain event ($p = 1)$ brings no information: \n",
    "\n",
    "$$\n",
    "-\\log(1) = 0\n",
    "$$\n",
    "\n",
    "- An event with probability $0$ brings infinite information (but it never happens...)\n",
    "- When two independent $s_i$ and $s_j$ events take place, their information gets added:\n",
    "  \n",
    "$$\n",
    "\\begin{align}\n",
    "i(s_i \\cap s_j) &= -log_2(p(s_i \\cap s_j) \\\\\n",
    "&= -log_2(p(s_i) \\cdot p(s_j) \\\\\n",
    "&= -log_2(p(s_i) - log_2(p(s_j)) \\\\\n",
    "&= i(s_i) + i(s_j)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### The choice of logarithm\n",
    "\n",
    "Using the logarithm function in the definition is crucial, \n",
    "since it is responsible for most of these properties.\n",
    "In particular, the fact that logarithm transforms a product into a sum\n",
    "allws to sum the informations of independent events.\n",
    "\n",
    "Any base of logarithm can be used in the definition, not just base 2, \n",
    "and all the consequences still hold.\n",
    "\n",
    "  - By convention, we typically use the binary logarithm $\\log_2()$.\n",
    "    In this case, the information $i(s_i)$ is measured in **bits**\n",
    "\n",
    "  - We could use instead the natural logarithm $ln()$, and the result\n",
    "    is measured in *nats*.\n",
    "\n",
    "The choice of the algorithm is not critical and doesn't change anything fundamental,\n",
    "since logarithm bases can always be converted to/from one another:\n",
    "\n",
    "$$\n",
    "\\log_b(x) = \\frac{\\log_a(x)}{\\log_a(b)}\n",
    "$$\n",
    "\n",
    "This means that information defined using different logarithms differ only by a scaling factor:\n",
    "\n",
    "$$\n",
    "i_b(s_i) =\\frac{i_a(s_i)}{\\log_a(b)}\n",
    "$$\n",
    "\n",
    "Following the convention of the scientific literature, we shall use the base 2 logarithm $log_2()$ from now on.\n",
    "\n",
    "\n",
    "### Information Source\n",
    "\n",
    "```{margin}\n",
    "Example:\n",
    "  - a football team can only win / lose / draw a match\n",
    "  \n",
    "There are 3 possible events, and no other options.\n",
    "```\n",
    "\n",
    "A probabilistic event is always part of a set of multiple events, \n",
    "containing all the possible outcomes which can happen at a given time.\n",
    "\n",
    "Each event (known as \"message\") has its own probability, which are all known beforehand.\n",
    "At a given time, only one of the events can happen. \n",
    "This carries the information that **it** happened (out of all possible events),\n",
    "and the quantity information quantity is dependent of the probability.\n",
    "\n",
    "We define an **information source** as the set of all events,\n",
    "together with their probabilities.\n",
    "\n",
    "The set of all messages forms the \"*alphabet*\" of the source.\n",
    "\n",
    "When an event takes place, we say that \"the information source generates a message\".\n",
    "\n",
    "#### Sequence of messages\n",
    "\n",
    "We are very rarely interested in a single message. \n",
    "Instead, we are interested analyzing large amounts of messages.\n",
    "\n",
    "An information source creates a **sequence of messages**, by generating\n",
    "messages one after another, randomly, according to the known probabilities.\n",
    "\n",
    "  - e.g. like throwing a coin or a dice several times in a row\n",
    "\n",
    "Depending on how the messages are generated, we distinguish between two types\n",
    "of information sources:\n",
    "\n",
    "1. **Memoryless** sources: each new message is generated independently on the previous messages\n",
    "2. Sources **with memory**: when generating a new message, the probabilities depend on one or more of the previous messages\n",
    "\n",
    "### Discrete memoryless source\n",
    "\n",
    "A **discrete memoryless source** (DMS) is an information source which produces a sequence of **independent** messages.\n",
    "The choice of a message at one time does not depend on the previous messages.\n",
    "Each message has a fixed probability, and every new message is generated randomly based on the probabilities.\n",
    "\n",
    "The set of probabilities is the **distribution** of the source, \n",
    "also known as a **probabilty mass function**.\n",
    "\n",
    "We represent a DMS as below, \n",
    "by giving it a name (\"S\"), listing the messages ($s_1, s_2, s_3$) and the probability distribution:\n",
    "\n",
    "$$\n",
    "\\sIII{S}{\\fIoII}{\\fIoIV}{\\fIoIV}\n",
    "$$\n",
    "\n",
    "A DMS is a discrete, complete and memoryless:\n",
    "\n",
    "  - **Discrete**: the set of messages is a discrete set\n",
    "  - **Complete**: the sum of all probabilities is 1, which means that one and only one event must take place at a given time:\n",
    "  $$\n",
    "  \\sum p(s_i) = 1\n",
    "  $$\n",
    "  - **Memoryless**: each message is independent of the previous messages\n",
    "\n",
    "A good example of a DMS is a coin, or a dice.\n",
    "\n",
    "One message generated by DMS is also called a **random variable** in probabilistics.\n",
    "\n",
    "```{admonition} Example\n",
    "\n",
    "A coin is a discrete memoryless source (DMS) with two messages:\n",
    "\n",
    "  $$\n",
    "  \\snII{S}{heads}{\\fIoII}{tails}{\\fIoII}\n",
    "  $$\n",
    "\n",
    "A dice is a discrete memoryless source (DMS) with six messages:\n",
    "\n",
    "  $$\n",
    "  \\sVI{S}{\\fIoVI}{\\fIoVI}{\\fIoVI}{\\fIoVI}{\\fIoVI}{\\fIoVI}\n",
    "  $$\n",
    "\n",
    "Playing the lottery can be modeled as DMS:\n",
    "\n",
    "  $$\n",
    "  \\sII{S}{0.9999}{0.0001}\n",
    "  $$\n",
    "\n",
    "An extreme type of DMS containing the certain event:\n",
    "\n",
    "  $$\n",
    "  \\sII{S}{1}{0}\n",
    "  $$\n",
    "\n",
    "Receiving an unknown *bit* (0 or 1) with equal probabilities:\n",
    "\n",
    "  $$\n",
    "  \\snII{S}{0}{\\fIoII}{1}{\\fIoII}\n",
    "  $$\n",
    "```\n",
    "\n",
    "#### Sequence of messages from DMS\n",
    "\n",
    "A DMS produces a sequence of messages by randomly selecting a message every time,\n",
    "with the same fixed probabilities, producing a sequence like:\n",
    "\n",
    "$$\n",
    "s_3 s_2 s_4 s_1 s_2 s_1 s_3 \\dots\n",
    "$$\n",
    "\n",
    "For example, throwing a dice several times in a row you can get a sequence \n",
    "\n",
    "$$\n",
    "4, 2, 3, 2, 1, 6, 1, 5, 4, 5 \\dots\n",
    "$$\n",
    "\n",
    "In a sequence which is very long, with length $N \\to \\infty$, \n",
    "each message $s_i$ appears in the sequence approximately $p(s_i) * N$ times.\n",
    "This gets more precise as $N$ gets larger.\n",
    "\n",
    "### Entropy of a DMS\n",
    "\n",
    "We usually don't care about the information of single message. We are interested in long sequences of messages\n",
    "(think millions of bits of data).\n",
    "\n",
    "```{margin}\n",
    "In general, the average value of any set of quantities $x_k$ is defined like\n",
    "\n",
    "$$\n",
    "\\overline{x} = \\sum_{k} p(x_k) \\cdot x_k\n",
    "$$\n",
    "```\n",
    "\n",
    "To analyze this in an easy manner, we need the *average* information of a message from a DMS.\n",
    "\n",
    "The **entropy of a DMS source** $S$ is **the average information of a message**:\n",
    "\n",
    "$$\n",
    "H(S) = \\sum_{k} p(s_k) i(s_k) = -\\sum_{k} p(s_k) \\log_2(p_k)\n",
    "$$\n",
    "where $p(s_k)$  is the probability of message $k$\n",
    "\n",
    "\n",
    "Since information of a message is measured in bits, \n",
    "entropy is measured in **bits** (or **bits / message**, to indicate it is an average value).\n",
    " \n",
    "Entropies using information defined with different logarithms base are differ only by a scaling factor:\n",
    "$$\n",
    "H_b(S) =\\frac{H_a(S)}{\\log_a(b)}\n",
    "$$\n",
    "\n",
    "\n",
    "```{admonition} Example\n",
    "\n",
    "Let's compute the entropies of some of the DMS defined above.\n",
    "\n",
    "Coin: \n",
    "\n",
    "  $$\n",
    "  H(S) = 1 \\textrm{ bit/message}\n",
    "  $$\n",
    "\n",
    "Dice: \n",
    "\n",
    "  $$\n",
    "  H(S) = \\log(6) \\textrm{  bits/message}\n",
    "  $$\n",
    "\n",
    "Lottery: \n",
    "\n",
    "  $$\n",
    "  H(S) = -0.9999 \\log(0.9999) - 0.0001 \\log(0.0001) \\textrm{ bits/message}\n",
    "  $$\n",
    "\n",
    "Receiving 1 bit with equal probabilities: \n",
    "\n",
    "  $$\n",
    "  H(S) = 1 \\textrm{ bit/message}\n",
    "  $$\n",
    " (hence the name!)\n",
    "```\n",
    "\n",
    "#### Interpretation of the entropy\n",
    "\n",
    "The entropy of an information source S is a fundamental quantity. \n",
    "It allows us to compare two different sources, which model\n",
    "different scenarios from real life.\n",
    "\n",
    "All the following interpretations of entropy are true:\n",
    "\n",
    "  - $H(S)$ is the **average uncertainty** of the source S\n",
    "\n",
    "  - $H(S)$ is the **average information** of a message from source S\n",
    "\n",
    "  - A very long sequence of $N$ messages generated by source S has total information $\\approx N \\cdot H(S)$\n",
    "\n",
    "We shall see in Chapter III that the entropy $H(S)$ says something very important \n",
    "about the **number of bits** requires to represent data in binary form:\n",
    "\n",
    "  - $H(S)$ is the minimum number of bits ($0$, $1$) required to uniquely represent a message from source S, on average\n",
    "\n",
    "  - A very long sequence of $N$ messages generated by source S needs at least $\\approx N \\cdot H(S)$ bits in order to be represented in binary form\n",
    "\n",
    "Thus, $H(S)$ is crucial when we discuss how to represent data efficiently.\n",
    "\n",
    "#### Properties of entropy\n",
    "\n",
    "We prove the following **properties of entropy**:\n",
    "\n",
    "1. $H(S) \\geq  0$ (non-negative)\n",
    "\n",
    "    Proof: via definition\n",
    "\n",
    "2. $H(S)$ is maximum when all $n$ messages have equal probability $\\frac{1}{n}$.\n",
    "The maximum value is $\\max H(S) = \\log(n)$\n",
    "\n",
    "    Proof: only for the case of 2 messages, use derivative in definition\n",
    "\n",
    "3. *Diversification* of the source always increases the entropy\n",
    "\n",
    "    Proof: compare entropies in both cases\n",
    "\n",
    "#### The entropy of a binary source\n",
    "\n",
    "Consider a general DMS with two messages:\n",
    "\n",
    "$$\n",
    "  \\sII{S}{p}{1-p}\n",
    "$$\n",
    "\n",
    "It's entropy is:\n",
    "\n",
    "$$\n",
    "H(S) = -p \\cdot \\log(p) - (1-p) \\cdot \\log(1-p)\n",
    "$$\n",
    "\n",
    "The entropy value as a function of $p$ is represented below:\n",
    "\n",
    "<!-- ![Entropy of a binary source](img/EntropyBinary.png){height=40%} -->\n",
    "\n",
    "![Entropy of a binary source](img/EntropyBinary.png)\n",
    "\n",
    "As an illustration of the property no.2 from above, \n",
    "we can see that the maximum entropy value of a DMS with two messages is reached \n",
    "when the two messages have the same probability, $p = 0.5$:\n",
    "\n",
    "$$\n",
    "p(s_1) = p = p(s_2) = 1 - p = 0.5\n",
    "$$\n",
    "\n",
    "and its value is:\n",
    "\n",
    "$$\n",
    "H_{max}(S) = 1\n",
    "$$\n",
    "\n",
    "#### Let's play games\n",
    "\n",
    "Let's analyze the following guessing games with the tools introduced until now.\n",
    "\n",
    "```{admonition} Exercise\n",
    "\n",
    "I think of a number between 1 and 8. You have to guess it by asking\n",
    "yes/no questions.\n",
    "\n",
    "  - How much uncertainty does the problem have?\n",
    "  - How is the best way to ask questions? Why?\n",
    "  - What if the questions are not asked in the best way?\n",
    "  - On average, what is the number of questions required to find the number?\n",
    "\n",
    "Now suppose I randomly choose a number according to the following distribution:\n",
    "  \n",
    "  $$\n",
    "  \\sIV{S}{\\fIoII}{\\fIoIV}{\\fIoVIII}{\\fIoVIII}\n",
    "  $$\n",
    "\n",
    "  - On average, what is the number of questions required to find the number? \n",
    "  - What questions would you ask?\n",
    "\n",
    "But what if the distribution is the following?\n",
    "  \n",
    "  $$\n",
    "  \\sIV{S}{0.14}{0.29}{0.4}{0.17}\n",
    "  $$\n",
    "\n",
    "Let's draw some general conclusions:\n",
    "\n",
    "  - What distribution makes guessing the number the most difficult?\n",
    "  - What distribution makes guessing the number the easiest?\n",
    "\n",
    "An **optimal decision tree** is best sequence of questions to ask in order to find the number with a minimum number\n",
    "of questions, reprsented as a binary tree graph. You will see examples when we solve the exercises.\n",
    "```\n",
    "#### Efficiency, redundancy, flow\n",
    "\n",
    "Using the $H(S)$, we define several other useful characteristics of a DMS.\n",
    "\n",
    "The **efficiency** of a DMS indicates how close is the entropy to its maximum possible value:\n",
    "  \n",
    "$$\n",
    "\\eta = \\frac{H(S)}{H_{max}} = \\frac{H(S)}{\\log(n)}\n",
    "$$\n",
    "\n",
    "The **redundancy** of a source is the remaining gap.\n",
    "\n",
    "- **Absolute** redundancy of a DMS:\n",
    "\n",
    "$$\n",
    "R = H_{max} - H(S)\n",
    "$$\n",
    "\n",
    "- **Relative** redundancy of a DMS:\n",
    "\n",
    "$$\n",
    "\\rho = \\frac{H_{max} - H(S)}{H_{max}} = 1 - \\eta\n",
    "$$\n",
    "\n",
    "Suppose that each message $s_i$ takes some time $t_i$ to be transmitted via some communication channel.\n",
    "The **information flow** of a DMS $S$ is the average information transmitted per unit of time:\n",
    "\n",
    "$$\n",
    "H_\\tau(S) = \\frac{H(S)}{\\overline{t}},\n",
    "$$\n",
    "\n",
    "where $\\overline{t}$ is the average duration of transmitting a message:\n",
    "\n",
    "$$\n",
    "\\overline{t} = \\sum_{i} p_i t_i \n",
    "$$\n",
    "\n",
    "The information flow is measured in **bps** (bits per second), and is important for data communication.\n",
    "\n",
    "#### The Kullback-Leibler distance\n",
    "\n",
    "```{sidebar} Example\n",
    "The KL distance is used to evaluate the performance of classification algorithms.\n",
    "Suppose we have a neural network algorithm trained to recognize if an image \n",
    "is showing a car, a human, or a dog.\n",
    "\n",
    "To test the algorithm, we give it an image of a dog. The algorithm outputs\n",
    "a probability score indicating what it believes the image contains:\n",
    "\n",
    "$$\n",
    "\\snIII{Result}{car}{0.23}{human}{0.08}{dog}{0.69}\n",
    "$$\n",
    "\n",
    "Here, the algorithm thinks it sees a dog, but is not very sure (only $69 \\%$ score).\n",
    "\n",
    "The ideal output for this image is $100 \\%$ score for the dog:\n",
    "\n",
    "$$\n",
    "\\snIII{Target}{car}{0}{human}{0}{dog}{1}\n",
    "$$\n",
    "\n",
    "The KL distance $D_{KL}(Target, Result)$ expresses as a single number \n",
    "the difference between the desired target and the algorithm's result.\n",
    "Thus is serves as a way to define the classification error.\n",
    "It can be used, for example, to compare different algorithms or to improve an existing algorithms.\n",
    "\n",
    "In machine learning terminology, the KL distance is better known as the \"*categorical cross-entropy*\".\n",
    "``` \n",
    "\n",
    "Suppose we have the following two DMS:\n",
    "\n",
    "\\begin{gather*}\n",
    "\\sIV{P}{0.14}{0.29}{0.4}{0.17}\n",
    "\\\\\n",
    "\\sIV{Q}{0.13}{0.33}{0.43}{0.11}\n",
    "\\end{gather*}\n",
    "\n",
    "The probability values of $P$ and $Q$ are close, so the two DMS\n",
    "are similar. But exactly how much similar?\n",
    "\n",
    "In many application we need a way to quantify how similar or how different\n",
    "are two probability distributions. The Kullback-Leibler distance \n",
    "(also known as *\"Kullback-Leibler divergence\"*, or *\"cross-entropy\"*, or *\"relative entropy\"*)\n",
    "is a way to quantify numerically how much different is one distribution from another one,\n",
    "  \n",
    "The **Kullback–Leibler (KL) distance** of two distributions P and Q  is\n",
    "\n",
    "$$\n",
    "D_{KL}(P,Q) = \\sum_i p(s_i) \\log(\\frac{p(s_i)}{q(s_i)})\n",
    "$$\n",
    "\n",
    "The two distributions must have the same number of messages.\n",
    "\n",
    "The KL distance provides a meaningful way to to measure the distance (difference) between two distributions.\n",
    "In many ways it provides the same intuitions as a geometrical distance:\n",
    "\n",
    "1. $D_{KL}(P, Q)$ is always $\\geq 0$, and is equal to $0$ only when P and Q are the same\n",
    "2. The higher $D_{KL}(P, Q)$ is, the more different the two distributions are\n",
    "\n",
    "However, one important property is not satisfied, and for this reason \n",
    "the KL distance is not proper distance function as defined e.g. in mathematical algebra.\n",
    "The KL distance is **not commutative**: :\n",
    "\n",
    "$$\n",
    "D_{KL}(P, Q) \\neq D_{KL}(Q, P)\n",
    "$$\n",
    "\n",
    "Despite this, it is widely used in applications.\n",
    "\n",
    "\n",
    "### Extended DMS\n",
    "\n",
    "The **n-th order extension** of a DMS $S$, represented as $S^n$,\n",
    "is a DMS which has as messages $\\sigma_i$\n",
    "all the combinations of $n$ messages of $S$:\n",
    "    \n",
    "$$\n",
    "\\sigma_i = \\underbrace{s_j s_k ... s_l}_{n}\n",
    "$$\n",
    "\n",
    "If $S$ has $k$ messages, $S^n$ has $k^n$ messages, \n",
    "\n",
    "Since $S$ is DMS, consecutive messages are independent of each other, \n",
    "and therefore their probabilities are multiplied:\n",
    "\n",
    "$$\n",
    "p(\\sigma_i) = p(s_j) \\cdot p(s_k) \\cdot ... \\cdot p(s_l)\n",
    "$$\n",
    "\n",
    "An example is provided below:\n",
    "\n",
    "$$\n",
    "\\sII{S}{\\fIoIV}{\\frac{3}{4}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\snIV{S^2}{\\sigma_1 = s_1 s_1}{\\frac{1}{16}}{\\sigma_2 = s_1 s_2}{\\frac{3}{16}}{\\sigma_3 = s_2 s_1}{\\frac{3}{16}}{\\sigma_4 = s_2 s_2}{\\frac{9}{16}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "S^3: \\left( \\begin{matrix} s_1 s_1 s_1 & s_1 s_1 s_2 & s_1 s_2 s_1 & s_1 s_2 s_2 & s_2 s_1 s_1 & s_2 s_1 s_2 & s_2 s_2 s_1 & s_2 s_2 s_2 \\\\ ... & ... & ... & ... & ... & ... & ... & ... \\end{matrix} \\right)\n",
    "$$\n",
    "\n",
    "Extended DMS are useful because they provide a way to group messages \n",
    "inside a sequence of messages.\n",
    "\n",
    "Suppose we have a long sequence of binary messages:\n",
    "\n",
    "$$\n",
    "0 1 0 0 1 1 0 0 1 1 1 0 0 1 0 0\n",
    "$$\n",
    "\n",
    "What kind of source generated this sequence?\n",
    "\n",
    "1. We can view it as a sequence of 16 messages generated from a source $S_1$ with two messages, $s_1 = 0$ and $s_2 = 1$\n",
    "2. We can group two bits, and view it as a sequence of 8 messages generated from a source $S_2$ with messages $00$, $01$, $10$, $11$\n",
    "3. We can group 8 bits into bytes, and view it is a sequence of 2 messages from a DMS $S_8$ which generates 256 bytes\n",
    "4. ... and so on\n",
    "\n",
    "There must be a connection between the DMS, no matter how we group the bits, since we're talking about the same binary sequence. \n",
    "The connections is that they are all just n-th order extensions of the initial binary DMS.\n",
    "\n",
    "#### Entropy of a DMS\n",
    "\n",
    "We now prove an important theorem about extended DMS.\n",
    "\n",
    "```{prf:theorem} Entropy of extended DMS\n",
    "\n",
    "The entropy of a $n$-th order extension is $n$ times larger than the entropy of the original DMS\n",
    "\n",
    "$$\n",
    "H(S^n) = n H(S)\n",
    "$$\n",
    "```\n",
    "\n",
    "Interpretation: grouping messages from a long sequence in blocks of $n$ does not change total information of a sequence.\n",
    "\n",
    "  - when we have a group of $N$ bits from a source $S$, total information is $\\approx N \\cdot H(S)$\n",
    "  - if we group  8 bits = 1 byte, we have $\\frac{N}{8}$ messages from a source $S^8$, total information is the same: $\\approx \\frac{N}{8} \\cdot 8 H(S)$\n",
    "\n",
    "This makes sense because we're talking about the same sequence, even if we group bits into half-bytes, bytes, 32-bit words etc.\n",
    "\n",
    "```{prf:proof}\n",
    "\n",
    "TBD. For now will be done in class.\n",
    "\n",
    "```\n",
    "\n",
    "### DMS as models for language generation\n",
    "\n",
    "We use information sources as mathematical models for real-life data generation and analysis. \n",
    "A straightforward example in in text analysis, since text is basically a sequence of graphical symbols\n",
    "(letters and punctuation signs), similar to a sequence of messages from an information source.\n",
    "\n",
    "Is a DMS a good model for text? Let's take the following example, for the English language.\n",
    "\n",
    "The probability distribution of letters in English (26 letters, ignoring capitalization) is given below:\n",
    "\n",
    "```{margin}\n",
    "Images are taken from the book \"*Elements of Information Theory\" by Cover, Thomas)*\"\n",
    "```\n",
    "\n",
    "<!-- ![](img/EngLetterProb.jpg){width=30%}\\  -->\n",
    "![](img/EngLetterProb.jpg)\n",
    "\n",
    "We consider a DMS whhich has the 26 letters as messages, with these probabilities.\n",
    "\n",
    "Generating a sequence of letters from this DMS produces the following:\n",
    "\n",
    "<!-- ![](img/EnglishFirstOrder.png){width=50%}\\ -->\n",
    "![](img/EnglishFirstOrder.png)\n",
    "\n",
    "This doesn't look like English. What's wrong? \n",
    "\n",
    "A DMS is **memoryless**, which means that every message is generated irrespective\n",
    "of the previous ones. This is not a good model for a text in a language. In a real language, \n",
    "the frequency of letter depends a lot on the previous letters:\n",
    "\n",
    "- `a` is a common letter in English (probability $8.2 \\%$), but if the previous letter is also `a`, \n",
    "  the probability is close to zero because the group `aa` is extremely rare\n",
    "- similarly, `h` has a much higher probability if the previous letter is `t` then if the previous letter is `x`\n",
    "\n",
    "The DMS is not capturing the dependencies between letters, because the memoryless property\n",
    "makes it very restrictive. We need to consider sources with memory.\n",
    "\n",
    "### Sources with memory\n",
    "\n",
    "```{prf:definition}\n",
    "A source has **memory of order $m$** if the probability\n",
    "of a message depends on the last $m$ messages.\n",
    "```\n",
    "\n",
    "The last $m$ messages define the **state** of the source (denoted as $S_i$).\n",
    "We say that the source \"is in the state $S_i$\".\n",
    "\n",
    "A source with $n$ messages and memory $m$ has a number of states equal to $n^m$.\n",
    "\n",
    "The source generates messages randomly, but with different message probabilities \n",
    "depending in which state the source is.\n",
    "We use the following notation: \n",
    "\n",
    "$p(s_i | S_k)$ = probability of message $s_i$ in state $S_k$\n",
    "\n",
    "Sources with memory are also known as *Markov sources*.\n",
    "\n",
    "```{admonition} Example\n",
    "The folllwing is a source with $n=4$ messages and memory $m=1$\n",
    "\n",
    "- if last message was $s_1$, choose next message with distribution\n",
    "\n",
    "$$\n",
    "\\sIV{S_1}{0.4}{0.3}{0.2}{0.1}\n",
    "$$\n",
    "\n",
    "- if last message was $s_2$, choose next message with distribution\n",
    "\n",
    "$$\n",
    "\\sIV{S_2}{0.33}{0.37}{0.15}{0.15}\n",
    "$$\n",
    "\n",
    "- if last message was $s_3$, choose next message with distribution\n",
    "\n",
    "$$\n",
    "\\sIV{S_3}{0.2}{0.35}{0.41}{0.04}\n",
    "$$\n",
    "\n",
    "- if last message was $s_4$, choose next message with distribution\n",
    "\n",
    "$$\n",
    "\\sIV{S_4}{0.1}{0.2}{0.3}{0.4}\n",
    "$$\n",
    "```\n",
    "\n",
    "\n",
    "#### Transition matrix\n",
    "\n",
    "When a new message is provided, the source **transitions** to a\n",
    "new state:\n",
    "\n",
    "$$\n",
    "...\\underbrace{s_i s_j s_k}_{\\text{old state}} s_l\n",
    "$$\n",
    "\n",
    "$$\n",
    "...s_i \\underbrace{s_j s_k s_l}_{\\text{new state}}\n",
    "$$\n",
    "\n",
    "\n",
    "Therefore we can view the probabilities of messages $s_i$ as **transition probabilities** from\n",
    "some state $S_u$ to another state $S_v$s.\n",
    "\n",
    "The transition probabilities are organized in a **transition matrix** $[T]$\n",
    "of size $N \\times N$, where $N$ is the total number of states.\n",
    "\n",
    "$$\n",
    "[T] = \n",
    "\\begin{bmatrix}\n",
    "p_{11} & p_{12} & ... & p_{1N} \\\\ \n",
    "p_{21} & p_{22} & ... & p_{2N} \\\\ \n",
    "... & ... & ... & ... \\\\ \n",
    "p_{N1} & p_{N2} & ... & p_{NN} \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The element $p_{ij}$ from $[T]$, located on row $i$ and column $j$,\n",
    "is the transition probability from state $S_i$ to state $S_j$.\n",
    "\n",
    "```{admonition} Exercise\n",
    "Write the transition matrix $[T]$ for the previous example\n",
    "```\n",
    "\n",
    "#### Graphical representation of sources with memory\n",
    "\n",
    "The transition matrix which defines a source with memory can be represented graphically,\n",
    "as a directed graph where the vertices are the states, and the edges are the transitions.\n",
    "Every edge (transition) has a certain probability,\n",
    "\n",
    "At whiteboard: draw states and transitions for previous example \n",
    "(source with $n=4$ messages and memory $m=1$)\n",
    "\n",
    "### Entropy of sources with memory\n",
    "\n",
    "How to compute the entropy of a source with memory?\n",
    "\n",
    "Note that each state $S_k$ has a different distribution, so each state can be viewed\n",
    "as a kind of DMS. Therefore we can compute an entropy $H(S_k)$ for every state $S_k$, \n",
    "using the same formula as for DMS:\n",
    "\n",
    "$$\n",
    "H(S_k) = - \\sum_i p(s_i | S_k) \\cdot \\log(p(s_i | S_k))\n",
    "$$\n",
    "\n",
    "However, the source moves from state to state, and it can spend more time\n",
    "in a state than in another one. How to define the global entropy?\n",
    "\n",
    "The global entropy of a source with memory is **the average entropy of the states**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(S) &= \\sum_k p_k H(S_k) \\\\\n",
    "&= - \\sum_k p_k \\sum_i p(s_i | S_k) \\cdot \\log(p(s_i | S_k)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The probabilities $p_k$ are known as the **stationary probabilities**,\n",
    "and they represent the probability that the source is in state $S_k$ at a given moment.\n",
    "\n",
    "Considering that the source operates for a very long time and generates a very long sequence of messages, \n",
    "you can think of $p_k$ as the fraction of time when the source was in state $S_k$.\n",
    "\n",
    "#### Ergodic sources\n",
    "\n",
    "How to find out the weights $p_k$?\n",
    "\n",
    "To find this, we first need to answer the following question:\n",
    "    \n",
    "> If we know the state $S_k$ at time $n$, what will be the state at time $n+1$?\n",
    "    \n",
    "Let $p_i^{(n)}$ denote the probability that the source is in state $S_i$ at time $n$.\n",
    "The source generates a message. In what state will the source end up at time $n+1$?\n",
    "\n",
    "The probabilities of the states at time $n+1$, $p_i^{(n)}$, are found by multiplying with $T$\n",
    "\n",
    "```{margin} Exercise\n",
    "Prove this.\n",
    "```\n",
    "\n",
    "$$\n",
    "[p_1^{(n)}, p_2^{(n)}, ... , p_N^{(n)}] \\cdot [T] = [p_1^{(n+1)}, p_2^{(n+1)}, ... , p_N^{(n+1)}]\n",
    "$$\n",
    "\n",
    "After one additional message, at time $(n+2)$? Multiply once more with $T$:\n",
    "\n",
    "$$\n",
    "[p_1^{(n)}, p_2^{(n)}, ... , p_N^{(n)}] \\cdot [T] \\cdot [T] = [p_1^{(n+2)}, p_2^{(n+2)}, ... , p_N^{(n+2)}]\n",
    "$$\n",
    "\n",
    "For every new moment of time, we do one more multiplication with $T$.\n",
    "In general, if we start from time $0$, after $n$ messages, the probabilities that the source ends up in a certain state are:\n",
    "\n",
    "$$\n",
    "[p_1^{(0)}, p_2^{(0)}, ... , p_N^{(0)}] \\cdot [T]^{n} = [p_1^{(n)}, p_2^{(n)}, ... , p_N^{(n)}]\n",
    "$$\n",
    "\n",
    "However, in general we don't know the initial state or the initial probabilities.\n",
    "\n",
    "#### Ergodicity\n",
    "\n",
    "To work around the problem of the unknown initial state, we make use a property called \"*ergodicity*\".\n",
    "\n",
    "A source is called **ergodic** if every state can be reached from every state, in a finite number of steps.\n",
    "\n",
    "If an ergodic source runs for a very long time $M \\to \\infty$, it will go through all transitions and all states many times,\n",
    "and, eventually, the fraction of time it finds itself in a certain state $S_k$ stabilizes. \n",
    "This happens irrespective of what was the starting state. \n",
    "Intuitively, the initial state doesn't matter if the source will anyway travel\n",
    "through all states and transitions many times, as $M \\to \\infty$.\n",
    "\n",
    "```{margin} Counter-example\n",
    "Can you show why ergodicity is important for this? \n",
    "\n",
    "Think of a non-ergodic source where the initial state always matters, even as the number of generated messages $M \\to \\infty$.\n",
    "```\n",
    "\n",
    "We formalize this as the following property of an ergodic source with memory:\n",
    "\n",
    "  - For an ergodic source with memory, after many messages, the probabilities of the states *become stationary*, \n",
    "    i.e. thet converge to some fixed values, no matter what state the source started from initially).\n",
    "  \n",
    "    $$\\lim_{n \\to \\infty} [p_1^{(n)}, p_2^{(n)}, ... p_N^{(n)}] = [p_1, p_2, ... p_N]$$\n",
    "\n",
    "\n",
    "#### Finding the stationary probabilities\n",
    "\n",
    "The ergodicity property helps us find the values of the stationary probabilities.\n",
    "When $n$ is very large, after $n$ messages and after $n+1$ messages the probabilities are the same, \n",
    "and therefore the following equation holds:\n",
    "\n",
    "$$\n",
    "[p_1, p_2, ... p_N] \\cdot [T] = [p_1, p_2, ... p_N]\n",
    "$$\n",
    "\n",
    "Note that we dropped the time $^(n)$ or ^$^(n+1)$, since the values have converged to fixed values\n",
    "and the times doesn't matter anymore.\n",
    "\n",
    "This is an equation system in matrix form, with $M$ unknowns and $M$ equations.\n",
    "\n",
    "However, the system is rank-deficient, i.e. one row is actually a linear combination of the others.\n",
    "Because of this, one row of the system should be removed, and replaced with a new equation which reflects\n",
    "the fact that the sum of probabilities is 1:\n",
    "\n",
    "$$\n",
    "p_1 + p_2 + ... + p_N = 1\n",
    "$$\n",
    "\n",
    "With this new equation, we obtain a complete system, which has a unique set of solutions by solving the system.\n",
    "\n",
    "#### Example: modelling English\n",
    "\n",
    "```{margin}\n",
    "This example is taken from \"Elements of Information Theory\" by Cover, Thomas\n",
    "```\n",
    "\n",
    "Let us consider a sequence of information sources modelling English language,\n",
    "going progressively from the most rudimentary model (memoryless),\n",
    "to the most advanced (source with memory of large order).\n",
    "\n",
    "Let's look at a sample sequence of letters generated from these sources.\n",
    "\n",
    "1. Text generated by memoryless source with equal probabilities:\n",
    "\n",
    "<!-- ![](img/EnglishZeroOrder.png){width=40%} -->\n",
    "![](img/EnglishZeroOrder.png)\n",
    "\n",
    "2. A memoryless source, but the probabilities of each letter as the ones in English:\n",
    "\n",
    "<!-- ![](img/EnglishFirstOrder.png){width=40%}\\ -->\n",
    "![](img/EnglishFirstOrder.png)\n",
    "\n",
    "3. A source with memory of order $m=1$, i.e. frequency of letter pairs are as in English:\n",
    "\n",
    "<!-- ![](img/EnglishSecondOrder.png){width=40%}\\ -->\n",
    "![](img/EnglishSecondOrder.png)\n",
    "\n",
    "4. A source with memory $m=2$, i.e. the frequency of letter triplets as in English:\n",
    "\n",
    "<!-- ![](img/EnglishThirdOrder.png){width=40%}\\ -->\n",
    "![](img/EnglishThirdOrder.png)\n",
    "\n",
    "5. A source with memory $m=3$, frequency of 4-plets as in English:\n",
    "\n",
    "<!-- ![](img/EnglishFourthOrder.png){width=40%}\\ -->\n",
    "![](img/EnglishFourthOrder.png)\n",
    "\n",
    "Sources with more memory are able to capture better \n",
    "the statistical dependencies between the letters, and because \n",
    "of this the generated text looks more and more like English.\n",
    "\n",
    "### Working with information sources\n",
    "\n",
    "When using information sources to model a real-life process,\n",
    "you will encounter some typical use-cases.\n",
    "\n",
    "1. How to train an information source (e.g. find the probabilities)\n",
    "2. Generate sequences from a source\n",
    "3. Compute the probability of an existing sequence\n",
    "   \n",
    "#### Training an information source model\n",
    "\n",
    "By training a model we mean finding the correct value of the parameters.\n",
    "In our case, the parameters are the probabilities of messages.\n",
    "\n",
    "For simplicity, we consider the case of text analysis in a certain language (e.g. English). \n",
    "\n",
    "First, we need a large sample of text, representative for the language.\n",
    "\n",
    "##### How to find the probabilities of a DMS?\n",
    "\n",
    "Go through your sample text, count the occurrences of every distinct character, \n",
    "  then divide the counters to the total number of characters. \n",
    "  You obtain the probabilities of each individual characters.\n",
    "\n",
    "##### How to find the probabilities of a source with memory of order $m$?\n",
    "\n",
    "A direct approach to find the transition matrix $[T]$ is as follows:\n",
    "\n",
    "- First, define each possible state $S_k$. Let's assume there are $N$ states.\n",
    "- Go though the sample text, count every occurrence of a group of $(m+1)$ distinct characters. \n",
    "  Place the counters in an $N \\times N$ matrix $[T]$, where each row corresponds to the old state (first $m$ characters of the group), and columns are the new state (last $m$ characters of the group).\n",
    "- Normalize each row: divide each row to the sum of the row. This ensures that the resulting row sums up to 1, i.e. it forms a probability distribution.\n",
    "\n",
    "One drawback is the huge memory requirements required to count all the possible combinations.\n",
    "The memory requirement increases at least exponentially with the memory order $m$.\n",
    "For example, if there are 26 letters, a source with memory of order $m = 3$ has $26^3 = 17576$ states,\n",
    "while a source with $m=4$ has $26^4 = 456976$ states. Considering capital letters and punctuation signs,\n",
    "the numbers are much larger. This makes it very cumbersome to work with sources of large memory, at least\n",
    "with such a brute-force approach.\n",
    "\n",
    "#### Generating messages from a source\n",
    "\n",
    "Supposing we have a source, we generate sequences by generating messages one after another.\n",
    "\n",
    "For a DMS, we generate each message independently of all the previous ones.\n",
    "\n",
    "For a source with memory $m$, described by a transition matrix $T$, we generate a message\n",
    "according to the transition probabilities from the row of $T$ corresponding to the current state.\n",
    "Then we update the current state and repeat the process.\n",
    "\n",
    "For a source with memory, we must also specify how to generate the first $m$ messages, \n",
    "i.e. before we have the first $m$ previous messages which define a full state.\n",
    "\n",
    "#### Compute the probability of an existing sequence\n",
    "\n",
    "Suppose we have a sequence of messages, e.g.\n",
    "\n",
    "> Seq: wirtschaftwissenschaftler\n",
    "\n",
    "If we have an information source, how do we compute the probability that the sequence *Seq*\n",
    "was generated from the source?\n",
    "\n",
    "- For a DMS, simply multiply the probabilities of every letter in the sequence\n",
    "- For a DMS with memory $m$, look at every group of $(m+1)$ letters \n",
    "  and multiply the corresponding probabilities from the transition matrix $T$\n",
    "\n",
    "Multiplying many probabilities quickly results in a very small number, which makes it\n",
    "problematic on a digital device due to numerical errors.\n",
    "\n",
    "To avoid this, instead of probabilities we can work with the log-probabilities, i.e. $\\log(p)$, \n",
    "which allows for much more manageable values. Instead of multiplying probabilities,\n",
    "we sum the log-probabilities.\n",
    "\n",
    "$$\n",
    "\\being(gather*)\n",
    "P = p(w) \\cdot p(i) \\cdot p(r) \\cdot \\dots \\cdot p(r)\n",
    "\\log(P) = \\log(p(w)) + \\log(p(i)) + \\log(p(r)) + \\dots + \\log(p(r))\n",
    "\\end(gather*)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Example application\n",
    "\n",
    "- Suppose we receive a text with random missing letters\n",
    "- We need to fill the blanks with the appropriate letters\n",
    "- How?\n",
    "\n",
    "  - build a model: source with memory of some order\n",
    "  - fill the missing letter with the most likely letter given by the model\n",
    "\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "1. Consider a discrete source with memory, with the graphical representation given below.\n",
    "The states are defined as follows: $S_1: s_1s_1$, $S_2: s_1s_2$, $S_3: s_2s_1$, $S_4: s_2s_2$.\n",
    "\n",
    "\t![Graphical representation of the source](img/MemorySource3.png){.id width=35%}\n",
    "\n",
    "Questions:\n",
    "\n",
    "  a. What are the values of $x$ and $y$?\n",
    "  b. Write the transition matrix $[T]$;\n",
    "  c. Compute the entropy in state $S_4$;\n",
    "  d. Compute the global entropy of the source;\n",
    "  e. What are the memory order, $m$, and the number of messages of the source, $n$?\n",
    "  f. If the source is initially in state $S_2$, in what states and with what probabilities \n",
    "    will the source be after 2 messages?\n",
    "\n",
    "#### Chapter summary\n",
    "\n",
    "- Information of a message: $i(s_k) = -\\log_2(p(s_k))$\n",
    "\n",
    "- Entropy of a memoryless source: $H(S) = \\sum_{k} p_k i(s_k) = -\\sum_{k} p_k \\log_2(p_k)$\n",
    "\n",
    "- Properties of entropy:\n",
    "    \n",
    "    1. $H(S) \\geq  0$\n",
    "\n",
    "    2. Is maximum when all messages have equal probability ($H_{max}(S) = \\log(n)$)\n",
    "\n",
    "    3. *Diversfication* of the source always increases the entropy\n",
    "\n",
    "- Sources with memory: definition, transitions\n",
    "\n",
    "- Stationary probabilities of ergodic sources with memory:\n",
    "$[p_1, p_2, ... p_N] \\cdot [T] = [p_1, p_2, ... p_N]$, $\\sum_i p_i = 1$.\n",
    "\n",
    "- Entropy of sources with memory:\n",
    "$$H(S) = \\sum_k p_k H(S_k) = - \\sum_k p_k \\sum_i p(s_i | S_k) \\cdot \\log(p(s_i | S_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "64be5866348a232da44f43870e2796c39c14b207a06220a0410b3c2ab015a68b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
